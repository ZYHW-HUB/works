import os
import sys
import pymysql
import psutil
from collections import OrderedDict
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import torchvision.transforms as transforms
from torch.optim.lr_scheduler import StepLR
import json
from torch.cuda.amp import autocast, GradScaler
import multiprocessing
import time
from datetime import timedelta, datetime
import csv
from tqdm import tqdm
import numpy as np

# --------------------------
# å…¨å±€é…ç½®ï¼šç›´æ¥è¯»å–ç°åº¦å›¾åƒï¼Œä¸ä½¿ç”¨é¢„å¤„ç†æ•°æ®
# --------------------------
use_preprocessed = False  # å…³é—­é¢„å¤„ç†
# åŸå§‹å›¾åƒç›®å½•ï¼ˆå­˜æ”¾ç°åº¦å›¾åƒæˆ–å½©è‰²å›¾åƒï¼Œç¨‹åºä¼šè½¬æ¢ä¸ºç°åº¦ï¼‰
image_dir = "walk/deeplabv3-master/training_logs/model_eval_seq_pp2"

# --------------------------
# è‡ªå®šä¹‰è½¬æ¢ï¼šè¿™é‡Œä¸å†è½¬æ¢ä¸ºç±»åˆ«æ ‡ç­¾ï¼Œè€Œæ˜¯ç›´æ¥è½¬æ¢ä¸ºç°åº¦å›¾çš„ Tensor
# --------------------------
class ConvertToGray(object):
    def __call__(self, image):
        # image: PIL Image
        # è½¬æ¢ä¸ºç°åº¦å›¾ï¼ˆå•é€šé“ï¼‰
        image = image.convert("L")
        return transforms.ToTensor()(image)

# --------------------------
# 1. æ•°æ®åº“é…ç½®ä¸æ•°æ®è¯»å–
# --------------------------
db_config = {
    "host": "localhost",
    "port": 3306,
    "user": "root",
    "password": "123",
    "database": "scene",
    "charset": "utf8mb4"
}

# æ•°æ®åŠ è½½å‡½æ•°
def load_pair_list(db_config):
    try:
        conn = pymysql.connect(**db_config)
        cursor = conn.cursor()
        query = "SELECT left_id, right_id, winner FROM pp2 WHERE category = 'boring'"
        cursor.execute(query)
        return [(p[0], p[1], p[2]) for p in cursor.fetchall()]
    except pymysql.MySQLError as e:
        print(f"æ•°æ®åº“é”™è¯¯: {e}")
        return []
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'conn' in locals():
            conn.close()

# åŸå§‹å›¾åƒè·¯å¾„è·å–å‡½æ•°ï¼ˆç”¨äºéé¢„å¤„ç†æ¨¡å¼ï¼‰
def get_image_path(image_id):
    return os.path.join(image_dir, f"{image_id}_pred.png")

# --------------------------
# åˆ›å»ºè¯„åˆ†è¡¨ä¸æ’å…¥è¯„åˆ†å‡½æ•°ï¼ˆä¿®ç€ä¿®ç€è¢«é—²ç½®äº†ï¼‰
# --------------------------
def create_scores_table(db_config):
    try:
        conn = pymysql.connect(**db_config)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS image_scores_boring_S (
                image_id VARCHAR(255),
                score FLOAT,
                split VARCHAR(10),
                PRIMARY KEY (image_id, split)
            )
        """)
        conn.commit()
    except pymysql.MySQLError as e:
        print(f"æ•°æ®åº“é”™è¯¯: {e}")
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'conn' in locals():
            conn.close()

def insert_scores(db_config, scores, split, conn, cursor, batch_size=1000):
    try:
        query = "INSERT INTO image_scores_boring_S VALUES (%s, %s, %s) ON DUPLICATE KEY UPDATE score=VALUES(score)"
        items = [(str(k), v, split) for k, v in scores.items()]
        for i in range(0, len(items), batch_size):
            cursor.executemany(query, items[i:i+batch_size])
            conn.commit()
    except pymysql.MySQLError as e:
        print(f"æ•°æ®åº“é”™è¯¯: {e.args}")

# --------------------------
# 2. æ•°æ®é›†ï¼šç›´æ¥è¯»å–ç°åº¦å›¾åƒ
# --------------------------
class SegmentationPairDataset(Dataset):
    def __init__(self, pair_list, transform=None):
        self.pair_list = pair_list
        self.transform = transform
        # æ ‡ç­¾æ˜ å°„ï¼š "left" è¡¨ç¤ºå·¦è¾¹è·èƒœï¼Œè½¬æ¢ä¸º 1ï¼›"right" è¡¨ç¤ºå³è¾¹è·èƒœï¼Œè½¬æ¢ä¸º -1ï¼›"equal" è½¬æ¢ä¸º 0
        self.label_map = {"left": 1, "right": -1, "equal": 0}
        self.cache = OrderedDict()
        self.max_cache_size = 1200  # æ ¹æ®å†…å­˜å®¹é‡è°ƒæ•´

    def __len__(self):
        return len(self.pair_list)

    def __getitem__(self, idx):
        if idx in self.cache:
            self.cache.move_to_end(idx)
            return self.cache[idx]

        left_id, right_id, winner = self.pair_list[idx]
        
        # ç›´æ¥è¯»å–ç°åº¦å›¾åƒ
        left_path = get_image_path(left_id)
        right_path = get_image_path(right_id)
        if not all(os.path.exists(p) for p in [left_path, right_path]):
            raise FileNotFoundError(f"ç¼ºå¤±æ–‡ä»¶: {left_path} æˆ– {right_path}")
        img_left = Image.open(left_path).convert("L")  # è½¬æ¢ä¸ºç°åº¦å›¾
        img_right = Image.open(right_path).convert("L")
        if self.transform:
            img_left = self.transform(img_left)
            img_right = self.transform(img_right)

        label = torch.tensor(self.label_map[winner], dtype=torch.float)

        if len(self.cache) >= self.max_cache_size:
            self.cache.pop(next(iter(self.cache)))  # åˆ é™¤æœ€æ—©æ’å…¥çš„ç¼“å­˜
        self.cache[idx] = (img_left, img_right, label, left_id, right_id)
        return self.cache[idx]

# --------------------------
# 3. å†…å­˜ä¼˜åŒ–æ¨¡å‹
# --------------------------
class SiameseNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        # æ³¨æ„ï¼šè¾“å…¥é€šé“ä¸º 1ï¼Œå› ä¸ºè¯»å–çš„æ˜¯ç°åº¦å›¾åƒ
        self.conv = nn.Sequential(
            nn.Conv2d(1, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2)
        )
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Sequential(nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 1))
        
        self._init_weights()
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

    def forward_once(self, x):
        x = x.contiguous(memory_format=torch.channels_last)
        # å¦‚ä¸éœ€è¦èŠ‚çœæ˜¾å­˜ï¼Œå¯ä¸ä½¿ç”¨ checkpointï¼ˆæ­¤å¤„æ³¨é‡Šæ‰ï¼‰
        # x = torch.utils.checkpoint.checkpoint(self.conv, x)
        x = self.conv(x)
        x = self.avgpool(x)
        return self.fc(x.flatten(1))

    def forward(self, x1, x2):
        return self.forward_once(x1), self.forward_once(x2)

# --------------------------
# 4. åŠ¨æ€æŸå¤±è®¡ç®—
# --------------------------
def compute_loss(r1, r2, label, margin=0.5):
    loss = torch.tensor(0.0, device=r1.device)
    margin_loss = nn.MarginRankingLoss(margin=margin, reduction="sum")
    mse_loss = nn.MSELoss(reduction="sum")

    pos_mask = label == 1
    neg_mask = label == -1
    eq_mask = label == 0

    if pos_mask.any():
        loss += margin_loss(r1[pos_mask], r2[pos_mask], torch.ones_like(r1[pos_mask]))
    if neg_mask.any():
        loss += margin_loss(r2[neg_mask], r1[neg_mask], torch.ones_like(r2[neg_mask]))
    if eq_mask.any():
        loss += mse_loss(r1[eq_mask], r2[eq_mask])

    return loss / label.numel()

# --------------------------
# 5. è‡ªé€‚åº”ç¡¬ä»¶é…ç½®
# --------------------------
def hardware_config():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æ ¹æ® GPU/CPU å†…å­˜åŠ¨æ€è®¡ç®—æ‰¹æ¬¡å¤§å°
    if device.type == "cuda":
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        batch_size = min(64, int(total_mem * 0.7 // 0.25))
    else:
        avail_mem = psutil.virtual_memory().available / 1024**3
        batch_size = min(32, int(avail_mem * 0.6 // 0.15))
    
    grad_accum_steps = 2 if batch_size < 16 else 1
    
    return (
        device,
        max(batch_size, 8),  # ä¿è¯æœ€å°æ‰¹æ¬¡å¤§å°
        min(4, os.cpu_count() // 2),  # å·¥ä½œè¿›ç¨‹æ•°
        grad_accum_steps
    )

# --------------------------
# 6. è®­ç»ƒæµç¨‹
# --------------------------
if __name__ == '__main__':
    # # å¦‚æœä½¿ç”¨é¢„å¤„ç†æ•°æ®ï¼Œæ£€æŸ¥é¢„å¤„ç†ç›®å½•æ˜¯å¦å­˜åœ¨æˆ–ä¸ºç©ºï¼Œè‹¥éœ€è¦åˆ™é¢„å¤„ç†
    # if use_preprocessed:
    #     if not os.path.exists(label_dir) or len(os.listdir(label_dir)) == 0:
    #         print("é¢„å¤„ç†æ•°æ®ä¸å­˜åœ¨ï¼Œå¼€å§‹é¢„å¤„ç†...")
    #         preprocess_all_images(image_dir, label_dir, color_map)

    # æ—¥å¿—ä¸æ¨¡å‹ä¿å­˜ç›®å½•é…ç½®
    log_dir = "walk/deeplabv3-master/training_logs/siamese_boring"
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"training_log_{datetime.now().strftime('%Y%m%d%H%M')}.csv")
    model_dir = os.path.join(log_dir, "saved_models")
    os.makedirs(model_dir, exist_ok=True)

    # åˆå§‹åŒ–æœ€ä½³æŒ‡æ ‡è·Ÿè¸ªå™¨
    best_metrics = {
        'epoch': 0,
        'val_loss': float('inf'),
        'val_acc': 0.0,
        'train_acc': 0.0
    }

    # åˆ›å»º CSV æ—¥å¿—æ–‡ä»¶å¤´
    with open(log_file, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow([
            'epoch', 'train_loss', 'val_loss', 
            'train_acc', 'val_acc', 'epoch_time',
            'model_path'
        ])

    multiprocessing.freeze_support()

    # åˆå§‹åŒ–ç¡¬ä»¶é…ç½®
    device, batch_size, num_workers, grad_accum = hardware_config()
    
    # ä½¿ç”¨è½¬æ¢ï¼šè°ƒæ•´å°ºå¯¸å¹¶è½¬æ¢ä¸ºç°åº¦å›¾ï¼ˆå•é€šé“ï¼‰
    transform = transforms.Compose([
        transforms.Resize((128, 128)),
        transforms.ToTensor()  # è‹¥å›¾åƒå·²è½¬ä¸ºç°åº¦ï¼ŒToTensor()ä¼šè‡ªåŠ¨ç”Ÿæˆå•é€šé“ tensor
    ])

    # æ•°æ®åŠ è½½
    pair_list = load_pair_list(db_config)
    # dataset = SegmentationPairDataset(pair_list, transform if not use_preprocessed else None)
    dataset = SegmentationPairDataset(pair_list, transform)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_set, val_set = random_split(dataset, [train_size, val_size])

    train_loader = DataLoader(
        train_set,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        generator=torch.Generator(device='cpu'),
        persistent_workers=True,
        drop_last=True,
        prefetch_factor=1  # é™ä½é¢„åŠ è½½ï¼Œå‡å°‘å†…å­˜å‹åŠ›
    )

    val_loader = DataLoader(
        val_set,
        batch_size=max(batch_size // 2, 8),
        shuffle=False,
        num_workers=max(1, num_workers // 2),
        pin_memory=True
    )

    # æ¨¡å‹åˆå§‹åŒ–å¹¶åŠ è½½åˆ°è®¾å¤‡ï¼ˆä½¿ç”¨ channel_last å†…å­˜æ ¼å¼ï¼‰
    model = SiameseNetwork().to(device, memory_format=torch.channels_last)
    
    # å¦‚æœæ”¯æŒ torch.compile ä¸”é Windowsï¼Œåˆ™ç¼–è¯‘æ¨¡å‹ä»¥æå‡é€Ÿåº¦
    if sys.platform != "win32" and hasattr(torch, "compile"):
        model = torch.compile(model)
    else:
        print("torch.compile åœ¨ Windows å¹³å°ä¸å¯ç”¨ï¼Œè·³è¿‡ç¼–è¯‘ã€‚")
    
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    scaler = GradScaler(enabled=torch.cuda.is_available())
    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)
    model_save_path = os.path.join(model_dir, "siamese_model.pth")
    scores_save_path = os.path.join(log_dir, "scores.json")
    create_scores_table(db_config)

    # æ•°æ®åº“è¿æ¥
    try:
        conn = pymysql.connect(**db_config)
        cursor = conn.cursor()
    except pymysql.MySQLError as e:
        print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        exit(1)

    # è®­ç»ƒå¾ªç¯
    total_start = time.time()
    for epoch in range(12):
        epoch_start = time.time()
        model.train()
        optimizer.zero_grad()
        
        # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡æŒ‡æ ‡
        running_loss = 0.0
        running_correct = 0  # è®­ç»ƒæ­£ç¡®æ ·æœ¬æ•°
        total_samples = 0    # è®­ç»ƒæœ‰æ•ˆæ ·æœ¬æ•°
        train_scores = {}

        # è®­ç»ƒé˜¶æ®µ
        for i, (img_l, img_r, labels, ids_l, ids_r) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}", leave=False)):
            # æ•°æ®ä¼ è¾“ï¼šé‡‡ç”¨éé˜»å¡ä¼ è¾“å’Œ channels_last æ ¼å¼
            img_l = img_l.to(device, non_blocking=True, memory_format=torch.channels_last)
            img_r = img_r.to(device, non_blocking=True, memory_format=torch.channels_last)
            labels = labels.to(device, non_blocking=True)

            with autocast(enabled=torch.cuda.is_available()):
                out1, out2 = model(img_l, img_r)
                loss = compute_loss(out1, out2, labels) / grad_accum

                with torch.no_grad():
                    preds = torch.where(out1 > out2, 1, -1)
                    mask = labels != 0  # æ’é™¤å¹³å±€æ ·æœ¬
                    correct = (preds.flatten()[mask] == labels[mask]).sum().item()
                    running_correct += correct
                    total_samples += mask.sum().item()

            if torch.cuda.is_available():
                scaler.scale(loss).backward()
                if (i + 1) % grad_accum == 0:
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
            else:
                loss.backward()
                if (i + 1) % grad_accum == 0:
                    optimizer.step()
                    optimizer.zero_grad()

            running_loss += loss.item()

        train_acc = running_correct / total_samples if total_samples > 0 else 0.0

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0  # éªŒè¯æ­£ç¡®æ ·æœ¬æ•°
        val_total = 0    # éªŒè¯æœ‰æ•ˆæ ·æœ¬æ•°
        val_scores = {}
        
        with torch.inference_mode(), autocast(enabled=torch.cuda.is_available()):
            for img_l, img_r, labels, ids_l, ids_r in tqdm(val_loader, desc="Validation", leave=False):
                img_l = img_l.to(device, non_blocking=True, memory_format=torch.channels_last)
                img_r = img_r.to(device, non_blocking=True, memory_format=torch.channels_last)
                labels = labels.to(device, non_blocking=True)
                
                out1, out2 = model(img_l, img_r)
                val_loss += compute_loss(out1, out2, labels).item()

                with torch.no_grad():
                    preds = torch.where(out1 > out2, 1, -1)
                    mask = labels != 0  # æ’é™¤å¹³å±€æ ·æœ¬
                    val_correct += (preds.flatten()[mask] == labels[mask]).sum().item()
                    val_total += mask.sum().item()

        avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0.0
        val_acc = val_correct / val_total if val_total > 0 else 0.0

        # ä¿å­˜æ¯ä¸ª epoch çš„æ¨¡å‹
        epoch_model_path = os.path.join(model_dir, f"epoch_{epoch+1}.pth")
        torch.save(model.state_dict(), epoch_model_path)

        # æ›´æ–°æœ€ä½³æ¨¡å‹æŒ‡æ ‡
        if val_acc > best_metrics['val_acc'] or (val_acc == best_metrics['val_acc'] and avg_val_loss < best_metrics['val_loss']):
            best_metrics.update({
                'epoch': epoch + 1,
                'val_loss': avg_val_loss,
                'val_acc': val_acc,
                'train_acc': train_acc
            })
            torch.save(model.state_dict(), os.path.join(model_dir, "best_model.pth"))

        with open(log_file, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                epoch + 1, 
                running_loss / len(train_loader) if len(train_loader) > 0 else 0.0,
                avg_val_loss,
                train_acc,
                val_acc,
                time.time() - epoch_start,
                epoch_model_path
            ])

        epoch_time = time.time() - epoch_start
        print(f"Epoch {epoch+1}/12 | Train Loss: {running_loss/len(train_loader):.4f} | Val Loss: {avg_val_loss:.4f} | Train Acc: {train_acc:.2%} | Val Acc: {val_acc:.2%} | Time: {timedelta(seconds=int(epoch_time))}")

        scheduler.step()

    print(f"\nğŸ† æœ€ä½³æ¨¡å‹ï¼šEpoch {best_metrics['epoch']}")
    print(f"â— éªŒè¯é›†æŸå¤±: {best_metrics['val_loss']:.4f}")
    print(f"â— è®­ç»ƒå‡†ç¡®ç‡: {best_metrics['train_acc']:.2%}")
    print(f"â— éªŒè¯å‡†ç¡®ç‡: {best_metrics['val_acc']:.2%}")
    print(f"â— æ¨¡å‹è·¯å¾„: {os.path.join(model_dir, 'best_model.pth')}")

    total_time = timedelta(seconds=int(time.time() - total_start))
    print(f"\nè®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_time}")
    
    cursor.close()
    conn.close()
    
    with open(scores_save_path, 'w') as f:
        json.dump({'train_scores': train_scores, 'val_scores': val_scores}, f)
        print(f"è¯„åˆ†å·²ä¿å­˜åˆ° {scores_save_path}")
